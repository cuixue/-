# 我的三月份论文

## 阅读理解
【1】Deep mind 组在这篇论文中提供了两个公开的阅读理解数据集。同时考虑到一些命名实体不通过理解文章也能判断出来，此文提出了entity replacement 和 
    permutation. 这篇论文中提出了两种模型。第一种模型为attentive reader 即在阅读每一个词的时候都给其对于query的一个权重。另一种策略是对于query中的
    每一个词都去阅读一次文章。类比于人的话，在我们阅读的时候，一般不会阅读到问题的每个词的时候，都去阅读一次原文的。但是最终的结果。两个模型效果相当。
    attentive reader 的效果可能更好些。
    
    [Teaching Machines to Read and Comprehend(Nips15)]

【2】用了一个端到端的方法，loss是triplet-loss,考虑到了bit之间的冗余问题,使用了部分连接.激活函数使用的sigmoid函数,和分段函数.只是找triplet对,然后进行loss计算。在结构上,选取了类似 `NIN`(考虑怎么好)的想法,最后hash bit 时，没有进行全连接的方法，而是分着连接，好处是减少了冗余.

    [Simultaneous Feature Learning and Hash Coding with Deep Neural Networks(CVPR15)]
