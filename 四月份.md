【1】这篇论文提出了一种处理文本检索相关的attention机制,并且实验上证明了这种策略比直接取平均的效果好.attention机制可以很好的解决变长和但是这种attention 机制忽略了词序的,可是在现实生活中很多
文本相关的任务词序就是没那么重要的.
              
      [FEED-FORWARD NETWORKS WITH ATTENTION CAN SOLVE SOME LONG-TERM MEMORY PROBLEMS]

【2】两个句子在输入端有细小的差别,随着层数的增加,这种差别会被放大.因此,提出了一种模型,既有"bag-of-words"这种无序的方法速度,又有recursive这种可以学到语义结构的准确率.这篇论文认为,相比与文本的语义结构和是否有序,可能输入的分线性变换对于文本的理解更加重要.

      [Deep Unordered Composition Rivals Syntactic Methods for Text Classification]
      
【3】提出了一种非监督的方法,将几种异质的网络结构结合起来.之前的非监督方法比如word-embedding都是任务无关的.现在将监督信息和非监督信息都用起来.我怎么看着效果没咋好呢.网络的构建即节点和边.优化的方法惨开skip-gram的负采样方法.同时,了解到文本的初级表示确实可以用词embedding的均值,在数学上就是在优化同一空间文本的表示和所有词的表示距离最小.

      [PTE: Predictive Text Embedding through Large-scale Heterogeneous Text Networks]
  
  
 [注]短文本存在的一些问题:
 
        1）document-level word co-occurence suffer from the sparsity in short documents        
        2)CNN reduces the problem of word ambiguity through using the word orders in local context in the convolutional kernels
